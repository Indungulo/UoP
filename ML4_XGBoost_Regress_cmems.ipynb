{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dca23171-0e41-41ca-a2d3-3def8cb2cd66",
   "metadata": {},
   "source": [
    "Authors: L. Biermann (UoP)\n",
    "Credits: This code was originally developed for the MaRS Modules.\n",
    "License: This code is offered as free-to-use in the public domain, with no warranty.\n",
    "Date_V2: 25/02/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ccb9eb-2368-460a-9c90-cd738bbfc23c",
   "metadata": {},
   "source": [
    "# Shallow Machine Learning Models: XGBoost Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be4359-7113-44df-9db4-33eefaa5d8bf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" \n",
    "     style=\"font-size: 1.1em; padding: 10px; margin: 10px 0; text-align: center;\">\n",
    "    \n",
    "    XGBoost: gradient boosting ensemble method that builds decision trees sequentially, with each new tree correcting errors from previous ones to optimise a differentiable loss function and capture complex nonlinear relationships.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a395d4ad-9eff-4dba-9ac7-c07b9dc4d6a8",
   "metadata": {},
   "source": [
    "### Import Libraries including from `sklearn` for shallow ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff73e0-8237-4087-81ba-8e4e3bb4e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "# Data Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#ignorewarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c7afd-8af2-44ea-9afe-5c2861b63ecc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" \n",
    "     style=\"font-size: 1.8em; font-weight: bold; padding: 15px; margin: 10px 0; text-align: center; background-color: #d9edf7; border-color: #bce8f1; color: #31708f; border-radius: 8px;\">\n",
    "    Data Preprocessing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560a835-a995-4b9b-af90-8f954b37c4a9",
   "metadata": {},
   "source": [
    "### Option 1 to Load CSV files containing variables -- pandas' `read_csv function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fa81a-211a-48ff-ba71-bc709239ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data are imported in Dataframe format\n",
    "SSH = pd.read_csv(\"cmems_mod_glo_phy_my_0.083deg_P1D-m_SSH.csv\", comment='#') # tells pandas to ignore lines starting with '#'\n",
    "SST = pd.read_csv(\"cmems_mod_glo_phy_my_0.083deg_P1D-m_SST.csv\", comment='#')\n",
    "SSS = pd.read_csv(\"cmems_mod_glo_phy_my_0.083deg_P1D-m_SSs.csv\", comment='#')\n",
    "VEL = pd.read_csv(\"cmems_mod_glo_phy_my_0.083deg_P1D-m_VEL.csv\", comment='#')\n",
    "MLD = pd.read_csv(\"cmems_mod_glo_phy_my_0.083deg_P1D-m_MLD.csv\", comment='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67e03a-dad7-41e3-9de3-6e5c17a3bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The UTC format that our dates (times) are currently in:\n",
    "SSH['dates'] = pd.to_datetime(SSH['time'], format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True)\n",
    "\n",
    "# Step1: convert to a more friendly format, add 'dates' as new column \n",
    "SSH['dates'] = SSH['dates'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "# Step2: Remove the times in this case as they contain no real information\n",
    "SSH['dates'] = pd.to_datetime(SSH['dates'])\n",
    "# Step3: Drop the now unnecessary 'time' column\n",
    "SSH = SSH.drop(columns=['time'])\n",
    "\n",
    "# Re-order your columns so date is still first\n",
    "SSH = SSH[['dates','zos']]\n",
    "# Display\n",
    "print(SSH.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50eacc-7968-49b8-869a-b5c0c113f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now combine these different vars to make a new df\n",
    "df = pd.DataFrame({'Date':SSH['dates'], 'SSH':SSH['zos'], 'SST':SST['thetao'], 'SSS':SSS['so'], \n",
    "                   'Vuo':VEL['uo'], 'Vvo':VEL['vo'], 'MLD':MLD['mlotst']})\n",
    "print(df.head(3))\n",
    "print(':')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdadede-b0f2-4bca-a587-aa10e18980fc",
   "metadata": {},
   "source": [
    "### Option 2 to Load CSV files containing variables -- `glob` and pandas' `read_csv function`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e71a1b77-92e6-4232-96a0-12c53c2a4550",
   "metadata": {},
   "source": [
    "Though more complicated, it is an easier way to load data if you have loads of files/ filenames change routinely"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f92e43d2-5573-4e10-bcc5-0af5c9da5266",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the file pattern to match relevant CSV files\n",
    "file_pattern = \"cmems_mod_glo_phy_my_0.083deg_P1D-m_*.csv\"\n",
    "\n",
    "# Read all matching CSV files into a dictionary\n",
    "df_dict = {}\n",
    "for file in glob.glob(file_pattern):\n",
    "    var_name = file.split(\"_\")[-1].split(\".\")[0]  # Extract variable name from filename\n",
    "    df_dict[var_name] = pd.read_csv(file, comment='#')\n",
    "\n",
    "# Ensure SSH is processed correctly (dates + renaming 'zos' to 'SSH')\n",
    "SSH = df_dict.get(\"SSH\")\n",
    "if SSH is not None:\n",
    "    SSH['dates'] = pd.to_datetime(SSH['time'], format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True)\n",
    "    SSH['dates'] = SSH['dates'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "    SSH['dates'] = pd.to_datetime(SSH['dates'])\n",
    "    SSH = SSH.drop(columns=['time'])\n",
    "    SSH = SSH.rename(columns={'zos': 'SSH'})  # Rename 'zos' to 'SSH'\n",
    "    SSH = SSH[['dates', 'SSH']]  # Keep 'dates' and SSH variable\n",
    "\n",
    "# Mapping of expected variables to their corresponding column names in CSVs\n",
    "data_vars = {\n",
    "    \"SST\": \"thetao\",\n",
    "    \"SSS\": \"so\",\n",
    "    \"Vuo\": \"uo\",\n",
    "    \"Vvo\": \"vo\",\n",
    "    \"MLD\": \"mlotst\"}\n",
    "\n",
    "# Merge all datasets dynamically\n",
    "df = SSH.copy() if SSH is not None else pd.DataFrame()\n",
    "\n",
    "for key, col in data_vars.items():\n",
    "    dataset_key = \"VEL\" if key in [\"Vuo\", \"Vvo\"] else key  # Ensure velocity data is accessed correctly\n",
    "    if dataset_key in df_dict:\n",
    "        df[key] = df_dict[dataset_key][col].values  # Assign values from each dataset\n",
    "\n",
    "# Print the first few rows to verify the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee1a7b-6f4b-4688-856e-d95cdd0b726a",
   "metadata": {},
   "source": [
    "### Set the `predictor` and `target` variables (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786007cb-4a98-495d-a374-2d1b0e3f9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['SSH', 'SSS', 'Vuo', 'Vvo', 'MLD'] # Predictor vars\n",
    "X = df[predictors].values \n",
    "y = df['SST'].values      # Target variable\n",
    "# Needs to be (n,n)(n,)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42294e7-ec41-4844-ae43-df3800780f12",
   "metadata": {},
   "source": [
    "### Split the data into two sets: `training` (80%) and `test` (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916045c-ba64-4fb2-a1a3-39c085fc725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your dataset so 20% is set aside for testing (0.2) \n",
    "# Set random_state to ensure yr train-test split is always the same (for reproducibility)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the 80% training: 20% testing split\n",
    "print(\"Trainin set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\",  X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b02411-1c1d-4046-a946-c67ec4de1c43",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" \n",
    "     style=\"font-size: 1.8em; font-weight: bold; padding: 15px; margin: 10px 0; text-align: center; background-color: #d9edf7; border-color: #bce8f1; color: #31708f; border-radius: 8px;\">\n",
    "    \n",
    "    XGBoost Regression Model\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a875b-b87b-43f2-a4b4-58f80998b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the XGBoost regressor with the squared error objective.\n",
    "xgb_reg = xgb.XGBRegressor(objective = 'reg:squarederror', random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f90075-4123-4843-98c2-bb8103de40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model on the training set\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict SST on the test dataset\n",
    "y_pred = xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e04ae2-d25a-4dc5-9f74-8b88e466fea3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" \n",
    "     style=\"font-size: 1.8em; font-weight: bold; padding: 15px; margin: 10px 0; text-align: center; background-color: #d9edf7; border-color: #bce8f1; color: #31708f; border-radius: 8px;\">\n",
    "    Evaluating Model Performance\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc828743-87c1-4107-a9aa-d829f09c7737",
   "metadata": {},
   "source": [
    "### Visualising Model Performance: simple `line plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022b9d9-85e2-4c51-a90c-b799538cf8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference\n",
    "diff = (y_pred - y_test)\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "xgb_df = pd.DataFrame({\n",
    "    'Modeled': y_pred,\n",
    "    'Actual': y_test,\n",
    "    'SST_diff': diff})\n",
    "\n",
    "# Reset index\n",
    "xgb_df.reset_index(drop=True, inplace=True)\n",
    "# Show df\n",
    "print(xgb_df.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd470c2b-c76a-49c0-8eaa-828db7330f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot findings using two subplots (2 rows, 1 column)\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10, 5))\n",
    "\n",
    "# Plot Actual and Modeled SST values\n",
    "ax[0].plot(xgb_df.index, xgb_df['Modeled'],linestyle='-', c='orangered', linewidth=1.5, label='Modeled')\n",
    "ax[0].plot(xgb_df.index, xgb_df['Actual'], linestyle='-', c='royalblue', linewidth=1.2, label='Actual')\n",
    "# Set the x and y axes\n",
    "ax[0].set_xlim(0, 68)  \n",
    "ax[0].set_ylim(22,32)\n",
    "# Format your line plot\n",
    "ax[0].grid(True, color='silver', linestyle=':', linewidth=0.7)\n",
    "ax[0].set_ylabel('SST')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot SST_diff values\n",
    "ax[1].plot(xgb_df.index, xgb_df['SST_diff'], linestyle=':', linewidth = 0.1, marker = 's', markersize = 5, \n",
    "           color = 'lightseagreen', label = 'Model minus Actual')\n",
    "# Set the x and y axes\n",
    "ax[1].set_xlim(0, 68)  \n",
    "ax[1].set_ylim(-4, 4)\n",
    "# Add thick line at 0 to highlight differences in SST\n",
    "ax[1].axhline(y = 0, color = 'grey', linewidth = 1.0)\n",
    "# Format your line plot\n",
    "ax[1].grid(True, color='silver', linestyle=':', linewidth = 0.5)\n",
    "ax[1].set_ylabel('SST Diff')\n",
    "ax[1].legend()\n",
    "\n",
    "# Adjust layout to prevent overlapping elements\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a9b1b-a97b-4794-8832-dfb9f6d3e2a3",
   "metadata": {},
   "source": [
    "### Metrics for XGBoost Model: `R2` and `RMSE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c6c37-b951-43ef-be20-98d5c5a27d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate xgb Model Performance - RMSE, R² score\n",
    "rmse= np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2  = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print output to 2 decimal places:\n",
    "print(f\"XGBoost R²  : {r2:.2f}\")\n",
    "print(f\"XGBoost RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "727a6e44-cfa8-4657-8a86-27c366af4cd0",
   "metadata": {},
   "source": [
    "Random Forest R² : 0.70\n",
    "Random Forest MSE: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771bb15-34a2-4e6d-bc3c-0d76359b5dcf",
   "metadata": {},
   "source": [
    "### That's not the improvement we were hoping for... "
   ]
  },
  {
   "cell_type": "raw",
   "id": "057ec90d-eb3d-488f-9a74-7695d3fa4237",
   "metadata": {},
   "source": [
    "Small differences in R² / RMSE may be due to how each model handles variance, noise, or parameter sensitivity on our specific datasets. While both RF and xgboost use decision trees, they build and combine them differently:\n",
    "-- RF builds many trees independently and averages their results. This often provides robust 'out the box' performance, especially if the       dataset is noisy or small. \n",
    "-- XGBoost uses boosting, where trees are built sequentially to correct previous errors. This method usually requires careful hyperparameter    tuning to avoid overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04afd8-5e99-4803-96cc-6ad687256de7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" \n",
    "     style=\"font-size: 1.8em; font-weight: bold; padding: 15px; margin: 10px 0; text-align: center; background-color: #d9edf7; border-color: #bce8f1; color: #31708f; border-radius: 8px;\">\n",
    "    Hyperparameter Tuning Using GridSearchCV\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b527f3b-02e9-4c13-bb39-fc30ffa7e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid (dictionary) with lists of possible values for each hyperparameter:\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],       # Step size shrinkage used to prevent overfitting\n",
    "    'max_depth': [3, 5, 7],                        # Maximum depth of a tree, controlling model complexity\n",
    "    'min_child_weight': [1, 3, 5],                 # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],             # Fraction of samples to use for each tree\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],      # Fraction of features to consider for each tree\n",
    "    'n_estimators': [100, 200, 300],               # Number of trees (boosting rounds) to build\n",
    "    'gamma': [0, 0.1, 0.5]                         # Minimum loss reduction required to make a split\n",
    "}\n",
    "\n",
    "# Initialise XGBoost regressor model with squared error objective.\n",
    "xgb_reg = xgb.XGBRegressor(objective = 'reg:squarederror', random_state = 42)\n",
    "\n",
    "# Set up RandomizedSearchCV with the following parameters:\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator = xgb_reg,                          # The base model to optimize (our XGBoost regressor)\n",
    "    param_distributions=param_grid,               # Dictionary with parameters to sample from (our grid)\n",
    "    n_iter = 100,                                 # Number of random parameter combinations to try (reduces total fits)\n",
    "    scoring='r2',                                 # Metric to evaluate performance (R² in this case)\n",
    "    cv = 5,                                       # Number of cross-validation folds (5-fold cross-validation)\n",
    "    verbose= 1,                                   # Verbosity level to print progress messages during the search\n",
    "    n_jobs =-1,                                   # Use all available CPU cores to parallelize the search\n",
    "    random_state = 42                             # Random seed for reproducibility of the random sampling\n",
    ")\n",
    "\n",
    "# Fit the randomised search on the training data:\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters and cross-validation (CV) scores\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print()\n",
    "print(\"Best CV R²: {:.2f}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b01d8-ec2d-4ac9-95dd-5eccb3261bdf",
   "metadata": {},
   "source": [
    "### That's better - we see a (modest) improvement from `R²`= 0.68 to `R²`= 0.71. We can now save this 'best' model using a built-in function `save_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932aa80-aa4c-415f-ac49-ac728524287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a JSON file\n",
    "random_search.best_estimator_.save_model(\"xgb_sst_model1.json\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb22f19d-9efa-4823-92b4-866b3d5c7874",
   "metadata": {},
   "source": [
    "Best Parameters Explanation:\n",
    "----------------------------\n",
    "subsample: 1.0\n",
    "All training samples are used for building each tree (no subsampling of rows).\n",
    "n_estimators: 200\n",
    "The model builds 200 trees (boosting rounds) -- seems to balance learning (good) with overfitting (avoid).\n",
    "min_child_weight: 3\n",
    "Minimum sum of instance weight of 3 is required in a child node before a split is made; helps prevent model from creating overly specific rules based on very few observations.\n",
    "max_depth: 5\n",
    "Each tree allowed a max depth of 5 -- controls model complexity to capture nonlinear relationships without overfitting.\n",
    "learning_rate: 0.05\n",
    "quite low learning rate so model learns slowly, steadily -- can improve generalisation when combined with higher no. of trees.\n",
    "gamma: 0.1\n",
    "A minimum loss reduction of 0.1 is required to make a split -- makes algorithm more conservative by avoiding splits that don’t improve the model substantially.\n",
    "colsample_bytree: 0.9\n",
    "For each tree, 90% of the features are randomly sampled -- introduces diversity among trees to help reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d7990-09f2-4af5-bc74-27004fa6bd4e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" \n",
    "     style=\"font-size: 1.8em; font-weight: bold; padding: 15px; margin: 10px 0; text-align: center; background-color: #d9edf7; border-color: #bce8f1; color: #31708f; border-radius: 8px;\">\n",
    "    Feature Importance\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcf74d-6e09-44c7-9bfe-48f1fd1776e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SHAP to explain our model predictions with XGBoost\n",
    "explainer = shap.TreeExplainer(random_search.best_estimator_)  # Use best_estimator_ from random_search\n",
    "shap_vals = explainer.shap_values(X_test)                      # Compute SHAP values for your test data\n",
    "\n",
    "# Get feature importance from the XGBoost model\n",
    "boost_importance = pd.Series(random_search.best_estimator_.feature_importances_,\n",
    "                             index = predictors) # predictors= variables (SSH, MLD, SSS)\n",
    "\n",
    "# Create df of the features and their importance (xgboost)\n",
    "boost_df = pd.DataFrame({\n",
    "    \"Variable\": predictors,\n",
    "    \"XGB Importance\": boost_importance})\n",
    "\n",
    "# Sort boost_df so most important variables are at the top (descending order)\n",
    "boost_df.sort_values(by = \"XGB Importance\", ascending = False, inplace = True)\n",
    "print(boost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90811d9c-8681-4750-b731-d9ba2a64b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean absolute SHAP values for each feature\n",
    "# This provides a robust measure of feature importance\n",
    "shap_importance = np.abs(shap_vals).mean(axis = 0)\n",
    "\n",
    "# Create df of the features and their importance (SHAP)\n",
    "shap_df = pd.DataFrame({\n",
    "    \"Variable\": predictors,\n",
    "    \"Mean Absolute SHAP\": shap_importance})\n",
    "\n",
    "# Sort shap_df so the most important features (variables) are at the top:\n",
    "shap_df.sort_values(by = \"Mean Absolute SHAP\", ascending = False, inplace = True)\n",
    "print(shap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ae97f-49ae-42c9-acfe-fb2e298870a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting SHAP:\n",
    "# Use the diverging \"Spectral\" palette for colormap:\n",
    "cmap = sns.color_palette(\"Spectral\", as_cmap = True)\n",
    "\n",
    "# Compute normalized ranking for each feature (btwn 0 - 1)\n",
    "# Note -- shap_df values used to determine relative order:\n",
    "norm_ranks = shap_df[\"Mean Absolute SHAP\"].rank(pct = True)\n",
    "\n",
    "# Map each normalized rank to a colour via colourmap:\n",
    "colors = norm_ranks.apply(lambda x: cmap(x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de01517-2fd4-4f45-b32f-cd4dbee82be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "# ----- Plot RF Feature Importance -----\n",
    "sns.barplot(data = boost_df, x = \"XGB Importance\", y = \"Variable\", ax = axes[0],\n",
    "    palette = colors)\n",
    "axes[0].set_title(\"Tuned XGBoost Feature Importance\", fontweight='bold')\n",
    "axes[0].set_xlabel(\"Tuned XGBoost Importance\", fontsize = 10)\n",
    "axes[0].set_ylabel(\"Variables\", fontsize = 10)\n",
    "axes[0].grid(axis = 'x', linestyle = '--', alpha = 0.7)\n",
    "\n",
    "# ----- Plot SHAP Feature Importance -----\n",
    "sns.barplot(data = shap_df, x = \"Mean Absolute SHAP\", y =\"Variable\", ax=axes[1],\n",
    "    palette = colors)\n",
    "axes[1].set_title(\"SHAP Feature Importance\", fontweight ='bold')\n",
    "axes[1].set_xlabel(\"Mean Absolute SHAP Value\", fontsize = 10)\n",
    "axes[1].set_ylabel(\"\")  # Remove redundant ylabel on the right plot\n",
    "axes[1].grid(axis = 'x', linestyle = '--', alpha = 0.7)\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510c031-c9b0-4d58-a376-2558b6bbee0b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\" \n",
    "     style=\"font-size: 1.em; padding: 15px; margin: 10px 0; text-align: left; background-color: #d9edf7; border-color: #bce8f1; color: #31708f; border-radius: 8px;\">\n",
    "\n",
    "    While the feature importance ranking is similar (MLD and SSH at the top), their scales and interpretations differ:\n",
    "    \n",
    "    ⦾ XGBoost Feature Importance \n",
    "       - Calculated based on the reduction in impurity (or gain) that each feature provides when used for splits.\n",
    "       - Normalized (0 - 1), so they represent the relative importance of features in splitting decisions.\n",
    "       - Rankings are more about the model's inner workings:\n",
    "       - i.e.: how frequently and effectively features are used to split nodes and reduce error during model training.\n",
    "        \n",
    "    ⦾ SHAP Mean Absolute Values\n",
    "       - Computed per sample as the contribution each feature makes to the model’s prediction, then averaged across all samples.\n",
    "       - Values are in the same units as the output variable (SST), and are not normalised in the same way as XGBoost.\n",
    "       - More about the actual impact of feature values:\n",
    "       - i.e.: how much each feature’s actual value contributed to the prediction (game theory foundation).\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e13ed7-c5d8-4d64-821c-ea70e76706a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
